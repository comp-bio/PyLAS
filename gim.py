# -*- coding: utf-8 -*-
import sys
from scipy.stats import entropy
from numpy.linalg import norm
import numpy as np

# TRAINED
trained_mean = 1902.053139
trained = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 2.807152484172046e-07, 2.807152484172046e-07, 0.0, 2.245721987337637e-06, 2.245721987337637e-06, 3.087867732589251e-06, 4.491443974675274e-06, 1.9650067389204325e-06, 3.087867732589251e-06, 3.3685829810064555e-06, 2.5264372357548416e-06, 2.245721987337637e-06, 3.087867732589251e-06, 4.772159223092479e-06, 2.807152484172046e-06, 1.9650067389204325e-06, 6.456450713595706e-06, 7.017881210430115e-06, 1.150932518510539e-05, 1.8246491147118297e-05, 2.2176504624959165e-05, 3.1159392574309714e-05, 4.800230747934199e-05, 6.119592415495062e-05, 0.00010330321141753129, 0.00012323399405515283, 0.00016758700330507116, 0.0002189578937654196, 0.0002871716991308003, 0.0003424726030689896, 0.0004426879467539317, 0.0005260603755338414, 0.0006619265557677685, 0.0007469832760381815, 0.0008376543012769387, 0.0009516246921343237, 0.0010549279035518549, 0.001151213233758956, 0.0012727629363236059, 0.0013698904122759585, 0.0014302441906856574, 0.0014990194265478726, 0.001518669493937077, 0.0015745318283721006, 0.0015964276177486427, 0.0016200076986156878, 0.0015700403843974253, 0.0015554431914797308, 0.0015411267138104535, 0.0014847029488785953, 0.0013951547846335068, 0.001397961937117679, 0.0012884829902349692, 0.001211286296920238, 0.0011169659734520572, 0.0010964737603176011, 0.0009695904680330247, 0.0008909901984762075, 0.0008174428033908999, 0.0007548433029938632, 0.000689998080609489, 0.0006487329390921599, 0.0005603076358407404, 0.0005038838709088822, 0.00044493366874126934, 0.00041995001163213817, 0.000391036341045166, 0.0003449990403047445, 0.00031271678673676595, 0.0002635916182637551, 0.00024057296789354435, 0.00021980003951067119, 0.00020436070084772496, 0.0001984656806309637, 0.00018162276572593138, 0.00016674485755981954, 0.0001456912139285292, 0.0001333397429981722, 0.00011733897383839154, 0.00011425110610580227, 0.00010049605893335926, 9.628533020710118e-05, 8.337242877990977e-05, 8.337242877990977e-05, 7.017881210430116e-05, 6.905595111063233e-05, 6.989809685588395e-05, 5.895020216761297e-05, 6.428379188753986e-05, 5.8388771670778564e-05, 5.052874471509683e-05, 4.4633724498335536e-05, 3.901941952999144e-05, 3.845798903315704e-05, 4.1265141517329084e-05, 3.5370121300567784e-05, 3.340511456164735e-05, 3.4527975555316166e-05, 2.779080959330326e-05, 2.7229379096468846e-05, 3.059796207747531e-05, 2.2457219873376368e-05, 2.301865037021078e-05, 2.0492213134455935e-05, 2.1615074128124756e-05, 2.386079611546239e-05, 2.751009434488605e-05, 2.0492213134455935e-05, 2.1053643631290344e-05, 1.8246491147118297e-05, 1.6842914905032277e-05, 1.7965775898701095e-05, 1.5439338662946254e-05, 1.880792164395271e-05, 1.7685060650283892e-05, 1.4597192917694641e-05, 1.4877908166111845e-05, 1.8246491147118297e-05, 1.094789468827098e-05, 1.572005391136346e-05, 1.3193616675608618e-05, 1.4316477669277435e-05, 1.8246491147118297e-05, 8.140742204098934e-06, 1.5439338662946254e-05, 1.2632186178774207e-05, 1.2632186178774207e-05, 1.1790040433522594e-05, 1.150932518510539e-05, 1.2632186178774207e-05, 1.3755047172443025e-05, 1.150932518510539e-05, 9.544318446184958e-06, 1.038646419143657e-05, 1.2070755681939798e-05, 1.2351470930357005e-05, 1.403576242086023e-05, 1.1790040433522594e-05, 1.094789468827098e-05, 8.982887949350547e-06, 7.579311707264524e-06, 9.82503369460216e-06, 1.038646419143657e-05, 8.982887949350547e-06, 9.263603197767753e-06, 9.263603197767753e-06, 7.86002695568173e-06, 5.895020216761297e-06, 9.82503369460216e-06, 9.82503369460216e-06, 1.094789468827098e-05, 7.579311707264524e-06, 7.579311707264524e-06, 7.86002695568173e-06, 6.737165962012911e-06, 8.982887949350547e-06, 1.038646419143657e-05, 9.263603197767753e-06, 7.86002695568173e-06, 4.772159223092479e-06, 7.017881210430115e-06, 8.702172700933343e-06, 5.614304968344092e-06, 8.702172700933343e-06, 7.017881210430115e-06, 5.895020216761297e-06, 4.772159223092479e-06, 8.702172700933343e-06, 5.895020216761297e-06, 8.140742204098934e-06, 7.017881210430115e-06, 4.772159223092479e-06, 6.456450713595706e-06, 5.895020216761297e-06, 6.175735465178502e-06, 6.175735465178502e-06, 7.017881210430115e-06, 7.2985964588473205e-06, 6.456450713595706e-06, 5.333589719926888e-06, 4.491443974675274e-06, 5.895020216761297e-06, 6.737165962012911e-06, 6.175735465178502e-06, 7.579311707264524e-06, 4.491443974675274e-06, 5.895020216761297e-06, 7.017881210430115e-06, 6.175735465178502e-06, 3.930013477840865e-06, 6.737165962012911e-06, 5.614304968344092e-06, 5.614304968344092e-06, 3.6492982294236602e-06, 5.614304968344092e-06, 5.052874471509683e-06, 4.210728726258069e-06, 5.052874471509683e-06, 6.737165962012911e-06, 4.772159223092479e-06, 5.052874471509683e-06, 4.491443974675274e-06, 2.807152484172046e-06, 4.491443974675274e-06, 3.3685829810064555e-06, 2.807152484172046e-06, 2.245721987337637e-06, 4.491443974675274e-06, 4.210728726258069e-06, 7.2985964588473205e-06, 5.052874471509683e-06, 3.930013477840865e-06, 3.087867732589251e-06, 5.333589719926888e-06, 5.052874471509683e-06, 5.895020216761297e-06, 5.895020216761297e-06, 5.052874471509683e-06, 4.210728726258069e-06, 3.6492982294236602e-06, 3.6492982294236602e-06, 2.807152484172046e-06, 4.491443974675274e-06, 5.052874471509683e-06, 3.6492982294236602e-06, 4.772159223092479e-06, 4.491443974675274e-06, 3.087867732589251e-06, 4.210728726258069e-06, 4.491443974675274e-06, 5.333589719926888e-06, 5.895020216761297e-06, 3.930013477840865e-06, 4.772159223092479e-06, 5.052874471509683e-06, 6.456450713595706e-06, 4.491443974675274e-06, 5.614304968344092e-06, 5.052874471509683e-06, 5.333589719926888e-06, 4.210728726258069e-06, 4.491443974675274e-06, 3.6492982294236602e-06, 3.3685829810064555e-06, 3.087867732589251e-06, 3.3685829810064555e-06, 5.614304968344092e-06, 4.210728726258069e-06, 3.930013477840865e-06, 3.930013477840865e-06, 4.491443974675274e-06, 4.210728726258069e-06, 5.052874471509683e-06, 3.6492982294236602e-06, 5.614304968344092e-06, 4.772159223092479e-06, 4.491443974675274e-06, 6.175735465178502e-06, 4.210728726258069e-06, 3.3685829810064555e-06, 4.491443974675274e-06, 4.210728726258069e-06, 3.6492982294236602e-06, 3.930013477840865e-06, 5.333589719926888e-06, 5.614304968344092e-06, 3.930013477840865e-06, 6.175735465178502e-06, 4.210728726258069e-06, 3.930013477840865e-06, 4.772159223092479e-06, 3.930013477840865e-06, 7.86002695568173e-06, 3.3685829810064555e-06, 4.772159223092479e-06, 6.456450713595706e-06, 4.210728726258069e-06, 4.772159223092479e-06, 5.052874471509683e-06, 3.930013477840865e-06, 5.333589719926888e-06, 5.052874471509683e-06, 6.737165962012911e-06, 4.210728726258069e-06, 5.614304968344092e-06, 3.087867732589251e-06, 5.614304968344092e-06, 4.772159223092479e-06, 4.210728726258069e-06, 4.491443974675274e-06, 3.087867732589251e-06, 5.333589719926888e-06, 5.333589719926888e-06, 3.6492982294236602e-06, 3.3685829810064555e-06, 4.772159223092479e-06, 6.175735465178502e-06, 6.737165962012911e-06, 4.772159223092479e-06, 5.614304968344092e-06, 5.614304968344092e-06, 4.491443974675274e-06, 8.140742204098934e-06, 8.982887949350547e-06, 8.421457452516139e-06, 5.052874471509683e-06, 6.456450713595706e-06, 5.333589719926888e-06, 6.456450713595706e-06, 7.2985964588473205e-06, 8.140742204098934e-06, 8.140742204098934e-06, 7.86002695568173e-06, 7.579311707264524e-06, 7.017881210430115e-06, 7.86002695568173e-06, 6.737165962012911e-06, 1.094789468827098e-05, 1.038646419143657e-05, 1.0667179439853777e-05, 1.2632186178774207e-05, 1.2632186178774207e-05, 1.2632186178774207e-05, 1.2351470930357005e-05, 1.4877908166111845e-05, 1.2632186178774207e-05, 1.1228609936688184e-05, 1.3474331924025822e-05, 1.2351470930357005e-05, 1.572005391136346e-05, 1.965006738920432e-05, 1.7965775898701095e-05, 1.572005391136346e-05, 2.301865037021078e-05, 2.1615074128124756e-05, 1.7685060650283892e-05, 2.1334358879707554e-05, 2.5264372357548414e-05, 2.8352240090137667e-05, 2.3299365618627982e-05, 2.9475101083806485e-05, 3.284368406481294e-05, 2.7229379096468846e-05, 3.2001538319561325e-05, 3.312439931323014e-05, 3.986156527524306e-05, 3.789655853632262e-05, 4.154585676574628e-05, 4.1265141517329084e-05, 4.323014825624951e-05, 4.519515499516994e-05, 5.47394734413549e-05, 5.6704480180275334e-05, 7.130167309796998e-05, 0.00010330321141753129, 0.00010021534368494205, 8.421457452516139e-07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
# TRAINED

def get_lines(path):
    with open(path) as f:
        content = f.readlines()
    for line in content:
        if line == "": continue
        yield line.replace('\n', '').replace(' ', '\t').split('\t')


range  = (500, 9000)
sample = get_lines(sys.argv[1])
header = next(sample)

values = [float(dict(zip(header, line))['fft']) for line in sample]
values = [max(min(v, range[1]), range[0]) for v in values if v > 0]
hist = np.histogram(values, bins=400, range=range, density=True)[0]
sign = 1

if trained_mean > np.mean(values) + 0.000001:
    sign = -1
    print("Attention! \n" + 
          "Your sample is less aberrant than the reference 'perfect' sequencing.\n" +
          "Metric values not applicable", file=sys.stderr)

_P = hist / norm(hist, ord=1)
_Q = trained / norm(trained, ord=1)
_M = 0.5 * (_P + _Q)
JSD = sign * 0.5 * (entropy(_P, _M) + entropy(_Q, _M))

print(JSD)
