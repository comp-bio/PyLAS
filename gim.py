# -*- coding: utf-8 -*-
import sys
from scipy.stats import entropy
from numpy.linalg import norm
import numpy as np

# TRAINED
trained_mean = 1677.733194
trained = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 2.931192097506105e-07, 2.931192097506105e-07, 0.0, 2.344953678004884e-06, 2.344953678004884e-06, 3.224311307256716e-06, 4.689907356009768e-06, 2.0518344682542734e-06, 3.224311307256716e-06, 3.517430517007326e-06, 2.6380728877554947e-06, 2.344953678004884e-06, 3.224311307256716e-06, 4.983026565760379e-06, 2.931192097506105e-06, 2.0518344682542734e-06, 6.741741824264042e-06, 7.327980243765263e-06, 1.2017887599775031e-05, 1.905274863378968e-05, 2.315641757029823e-05, 3.2536232282317765e-05, 5.01233848673544e-05, 6.38999877256331e-05, 0.00010786786918822467, 0.00012867933308051803, 0.00017499216822111448, 0.0002286329836054762, 0.00029986095157487455, 0.00035760543589574484, 0.00046224899377671284, 0.0005493053990726441, 0.0006911750965919396, 0.0007799902171463745, 0.0008746677218958219, 0.0009936741210545697, 0.0011015419902427944, 0.0012020818791872538, 0.001329002497009268, 0.0014304217435829793, 0.0014934423736793604, 0.00156525658006826, 0.001585774924750803, 0.0016441056474911743, 0.001666968945851722, 0.0016915909594707732, 0.0016394157401351645, 0.0016241735412281329, 0.0016092244615308518, 0.0015503075003709791, 0.0014568024724605341, 0.0014597336645580403, 0.0013454171727553023, 0.0012648093900738844, 0.0011663213355976793, 0.0011449236332858846, 0.0010124337504786086, 0.0009303603717484378, 0.0008535631387937778, 0.0007881975550193917, 0.0007204870175670006, 0.0006773984937336609, 0.0005850659426622185, 0.0005261489815023458, 0.00046459394745471767, 0.00043850633778691337, 0.0004083150591826004, 0.0003602435087835003, 0.0003265347996621801, 0.0002752389379558233, 0.0002512031627562732, 0.00022951234123472802, 0.00021339078469844445, 0.00020723528129368166, 0.000189648128708645, 0.00017411281059186265, 0.00015212886986056684, 0.00013923162463153998, 0.00012252382967575521, 0.00011929951836849847, 0.00010493667709071856, 0.0001005398889444594, 8.705640529593133e-05, 8.705640529593133e-05, 7.327980243765264e-05, 7.210732559865018e-05, 7.298668322790201e-05, 6.15550340476282e-05, 6.712429903288982e-05, 6.096879562812699e-05, 5.276145775510989e-05, 4.660595435034707e-05, 4.0743570155334864e-05, 4.015733173583364e-05, 4.308852383333975e-05, 3.693302042857693e-05, 3.488118596032265e-05, 3.605366279932509e-05, 2.9018801765310443e-05, 2.843256334580922e-05, 3.194999386281655e-05, 2.344953678004884e-05, 2.4035775199550062e-05, 2.1397702311794565e-05, 2.257017915079701e-05, 2.4915132828801893e-05, 2.872568255555983e-05, 2.1397702311794565e-05, 2.198394073129579e-05, 1.905274863378968e-05, 1.7587152585036633e-05, 1.875962942403907e-05, 1.612155653628358e-05, 1.9638987053290902e-05, 1.8466510214288464e-05, 1.5242198907031748e-05, 1.5535318116782356e-05, 1.905274863378968e-05, 1.143164918027381e-05, 1.6414675746034188e-05, 1.3776602858278695e-05, 1.4949079697281135e-05, 1.905274863378968e-05, 8.500457082767706e-06, 1.612155653628358e-05, 1.3190364438777473e-05, 1.3190364438777473e-05, 1.2311006809525642e-05, 1.2017887599775031e-05, 1.3190364438777473e-05, 1.4362841277779915e-05, 1.2017887599775031e-05, 9.966053131520758e-06, 1.084541076077259e-05, 1.2604126019276252e-05, 1.2897245229026864e-05, 1.4655960487530527e-05, 1.2311006809525642e-05, 1.143164918027381e-05, 9.379814712019536e-06, 7.914218663266483e-06, 1.0259172341271367e-05, 1.084541076077259e-05, 9.379814712019536e-06, 9.672933921770148e-06, 9.672933921770148e-06, 8.207337873017094e-06, 6.155503404762821e-06, 1.0259172341271367e-05, 1.0259172341271367e-05, 1.143164918027381e-05, 7.914218663266483e-06, 7.914218663266483e-06, 8.207337873017094e-06, 7.034861034014652e-06, 9.379814712019536e-06, 1.084541076077259e-05, 9.672933921770148e-06, 8.207337873017094e-06, 4.396788146259158e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
# TRAINED

def get_lines(path):
    with open(path) as f:
        content = f.readlines()
    for line in content:
        if line == "": continue
        yield line.replace('\n', '').replace(' ', '\t').split('\t')


range  = (500, 9000)
sample = get_lines(sys.argv[1])
header = next(sample)

values = []
for line in sample:
    obj, val = dict(zip(header, line)), float(line[3])
    if 'fft' in obj: val = float(obj['fft'])
    if 'FFT_dF' in obj: val = float(obj['FFT_dF'])
    if val < 0 or val > range[1]: val = range[1]
    if val < range[0]: val = range[0]
    values.append(val)

hist = np.histogram(values, bins=400, range=range, density=True)[0]
sign = 1

if trained_mean > np.mean(values) + 0.000001:
    sign = -1
    print("Attention! \n" + 
          "Your sample is less aberrant than the reference 'perfect' sequencing.\n" +
          "Metric values not applicable. File: (%s)" % sys.argv[1], file=sys.stderr)

_P = hist / norm(hist, ord=1)
_Q = trained / norm(trained, ord=1)
_M = 0.5 * (_P + _Q)
JSD = sign * 0.5 * (entropy(_P, _M) + entropy(_Q, _M))

print(JSD)
